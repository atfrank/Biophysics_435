{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\"the activation function of a node defines the output of that node given an input or set of inputs.\" -- Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about activation functions: \n",
    "    1. https://keras.io/api/layers/activations/\n",
    "    2. https://en.wikipedia.org/wiki/Activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow.python.keras.api._v1.keras.activations in tensorflow.python.keras.api._v1.keras:\n",
      "\n",
      "NAME\n",
      "    tensorflow.python.keras.api._v1.keras.activations\n",
      "\n",
      "DESCRIPTION\n",
      "    # This file is MACHINE GENERATED! Do not edit.\n",
      "    # Generated by: tensorflow/python/tools/api/generator/create_python_api.py script.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "FUNCTIONS\n",
      "    deserialize(name, custom_objects=None)\n",
      "    \n",
      "    elu(x, alpha=1.0)\n",
      "        Exponential linear unit.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "            alpha: A scalar, slope of negative section.\n",
      "        \n",
      "        Returns:\n",
      "            The exponential linear activation: `x` if `x > 0` and\n",
      "              `alpha * (exp(x)-1)` if `x < 0`.\n",
      "        \n",
      "        Reference:\n",
      "            - [Fast and Accurate Deep Network Learning by Exponential\n",
      "              Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)\n",
      "    \n",
      "    exponential(x)\n",
      "        Exponential activation function.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            The exponential activation: `exp(x)`.\n",
      "    \n",
      "    get(identifier)\n",
      "    \n",
      "    hard_sigmoid(x)\n",
      "        Hard sigmoid activation function.\n",
      "        \n",
      "        Faster to compute than sigmoid activation.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            Hard sigmoid activation:\n",
      "            - `0` if `x < -2.5`\n",
      "            - `1` if `x > 2.5`\n",
      "            - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`.\n",
      "    \n",
      "    linear(x)\n",
      "        Linear activation function.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            The linear activation: `x`.\n",
      "    \n",
      "    relu(x, alpha=0.0, max_value=None, threshold=0)\n",
      "        Rectified Linear Unit.\n",
      "        \n",
      "        With default values, it returns element-wise `max(x, 0)`.\n",
      "        \n",
      "        Otherwise, it follows:\n",
      "        `f(x) = max_value` for `x >= max_value`,\n",
      "        `f(x) = x` for `threshold <= x < max_value`,\n",
      "        `f(x) = alpha * (x - threshold)` otherwise.\n",
      "        \n",
      "        Arguments:\n",
      "            x: A tensor or variable.\n",
      "            alpha: A scalar, slope of negative section (default=`0.`).\n",
      "            max_value: float. Saturation threshold.\n",
      "            threshold: float. Threshold value for thresholded activation.\n",
      "        \n",
      "        Returns:\n",
      "            A tensor.\n",
      "    \n",
      "    selu(x)\n",
      "        Scaled Exponential Linear Unit (SELU).\n",
      "        \n",
      "        The Scaled Exponential Linear Unit (SELU) activation function is:\n",
      "        `scale * x` if `x > 0` and `scale * alpha * (exp(x) - 1)` if `x < 0`\n",
      "        where `alpha` and `scale` are pre-defined constants\n",
      "        (`alpha = 1.67326324`\n",
      "        and `scale = 1.05070098`).\n",
      "        The SELU activation function multiplies  `scale` > 1 with the\n",
      "        `[elu](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu)`\n",
      "        (Exponential Linear Unit (ELU)) to ensure a slope larger than one\n",
      "        for positive net inputs.\n",
      "        \n",
      "        The values of `alpha` and `scale` are\n",
      "        chosen so that the mean and variance of the inputs are preserved\n",
      "        between two consecutive layers as long as the weights are initialized\n",
      "        correctly (see [`lecun_normal` initialization]\n",
      "        (https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal))\n",
      "        and the number of inputs is \"large enough\"\n",
      "        (see references for more information).\n",
      "        \n",
      "        ![](https://cdn-images-1.medium.com/max/1600/1*m0e8lZU_Zrkh4ESfQkY2Pw.png)\n",
      "        (Courtesy: Blog on Towards DataScience at\n",
      "        https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9)\n",
      "        \n",
      "        Example Usage:\n",
      "        ```python3\n",
      "        n_classes = 10 #10-class problem\n",
      "        model = models.Sequential()\n",
      "        model.add(Dense(64, kernel_initializer='lecun_normal', activation='selu',\n",
      "        input_shape=(28, 28, 1))))\n",
      "        model.add(Dense(32, kernel_initializer='lecun_normal', activation='selu'))\n",
      "        model.add(Dense(16, kernel_initializer='lecun_normal', activation='selu'))\n",
      "        model.add(Dense(n_classes, activation='softmax'))\n",
      "        ```\n",
      "        \n",
      "        Arguments:\n",
      "            x: A tensor or variable to compute the activation function for.\n",
      "        \n",
      "        Returns:\n",
      "            The scaled exponential unit activation: `scale * elu(x, alpha)`.\n",
      "        \n",
      "        # Note\n",
      "            - To be used together with the initialization \"[lecun_normal]\n",
      "            (https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal)\".\n",
      "            - To be used together with the dropout variant \"[AlphaDropout]\n",
      "            (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout)\".\n",
      "        \n",
      "        References:\n",
      "            [Self-Normalizing Neural Networks (Klambauer et al, 2017)]\n",
      "            (https://arxiv.org/abs/1706.02515)\n",
      "    \n",
      "    serialize(activation)\n",
      "    \n",
      "    sigmoid(x)\n",
      "        Sigmoid.\n",
      "        \n",
      "        Applies the sigmoid activation function. The sigmoid function is defined as\n",
      "        1 divided by (1 + exp(-x)). It's curve is like an \"S\" and is like a smoothed\n",
      "        version of the Heaviside (Unit Step Function) function. For small values\n",
      "        (<-5) the sigmoid returns a value close to zero and for larger values (>5)\n",
      "        the result of the function gets close to 1.\n",
      "        Arguments:\n",
      "            x: A tensor or variable.\n",
      "        \n",
      "        Returns:\n",
      "            A tensor.\n",
      "        Sigmoid activation function.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            The sigmoid activation: `(1.0 / (1.0 + exp(-x)))`.\n",
      "    \n",
      "    softmax(x, axis=-1)\n",
      "        The softmax activation function transforms the outputs so that all values are in\n",
      "        \n",
      "        range (0, 1) and sum to 1. It is often used as the activation for the last\n",
      "        layer of a classification network because the result could be interpreted as\n",
      "        a probability distribution. The softmax of x is calculated by\n",
      "        exp(x)/tf.reduce_sum(exp(x)).\n",
      "        \n",
      "        Arguments:\n",
      "            x : Input tensor.\n",
      "            axis: Integer, axis along which the softmax normalization is applied.\n",
      "        \n",
      "        Returns:\n",
      "            Tensor, output of softmax transformation (all values are non-negative\n",
      "              and sum to 1).\n",
      "        \n",
      "        Raises:\n",
      "            ValueError: In case `dim(x) == 1`.\n",
      "    \n",
      "    softplus(x)\n",
      "        Softplus activation function.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            The softplus activation: `log(exp(x) + 1)`.\n",
      "    \n",
      "    softsign(x)\n",
      "        Softsign activation function.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            The softplus activation: `x / (abs(x) + 1)`.\n",
      "    \n",
      "    tanh(x)\n",
      "        Hyperbolic Tangent activation function.\n",
      "        \n",
      "        Arguments:\n",
      "            x: Input tensor.\n",
      "        \n",
      "        Returns:\n",
      "            The tanh activation: `tanh(x) = sinh(x)/cosh(x) = ((exp(x) -\n",
      "            exp(-x))/(exp(x) + exp(-x)))`.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['__builtins__', '__cached__', '__doc__', '__file__', '__loa...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/biophysics_435/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/keras/activations/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(activations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
